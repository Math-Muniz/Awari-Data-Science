# Leitura: Conceitos de Machine Learning
Representação de Machine Learning e os métodos de aprendizado, com tarefas e atividades comuns a cada um. Fonte: towardsdatascience.com

Machine Learning (ML) ou Aprendizado de Máquina é o campo que engloba o estudo, a criação e a utilização de algoritmos computacionais capazes de aprenderem e de melhorarem automaticamente, por meio do uso de dados.

De modo diferente da programação tradicional, em que algoritmos são “determinísticos”, isto é, comportam-se sempre da mesma maneira, algoritmos de Machine Learning partem de um determinado estado e buscam se adaptar e melhorar a partir dos dados que recebem, sem precisarem ser explicitamente programados para isso.

Um exemplo bastante simples é a classificação de imagens. Digamos que queiramos criar um algoritmo que classifique corretamente imagens de gatos e de cachorros. 

O processo mais comum é treinar um algoritmo com muitas fotos diferentes de gatos e de cachorros, para que ele “aprenda” as características (reconheça padrões comuns) dessas imagens.

Realizado o treinamento, então, podemos testar imagens novas e desconhecidas no algoritmo, para que ele classifique se são de gatos ou de cachorros.

Nos bastidores, tudo isso é feito com base em estatística e matemática, na forma dos algoritmos. O que parece “mágica” na verdade não passa, grosso modo, de cálculos de distâncias de pixels ou cores dessas imagens.

# Usos e aplicações de ML
Machine Learning está em muitas coisas que já usamos no dia a dia e em novidades promissoras:

* Sistemas de recomendação: quando a Netflix recomenda filmes e séries a seus clientes, com base nas preferências destes, são sistemas de recomendação que entram em ação. Eles também são bastante populares em e-commerces (recomendação de produtos) e redes sociais (recomendação de conteúdo).
* Chatbots e atendimento virtuais: chatbot e atendentes virtuais usam Machine Learning para entender a linguagem natural (usada por humanos) e formular respostas nela. Alguns são bastante simples e tem pouca naturalidade e flexibilidade. Outros, porém, podem conversar quase como um humano. A abordagem por trás dos casos mais avançados chama-se Processamento de Linguagem Natural (NLP).
* Reconhecimento facial: quando um aplicativo permite acesso a seus serviços por meio do reconhecimento do rosto do cliente, por exemplo, o que está em jogo são sistemas de reconhecimento facial, que usam algoritmos de Visão Computacional.
* Carros autônomos e robôs: ambos dependem de sistema de visão computacional e algoritmos de Machine Learning que processam dados recebidos de diversos sensores para saber como devem se comportar em ambientes do mundo real (se devem fazer uma curva ou desviar de um obstáculo à frente, por exemplo).
* Negociações automatizadas: há investidores e fundos de investimentos que empregam Machine Learning para monitorar o comportamento de mercados e realizar a compra de títulos ou ativos de maneira automatizada, sem necessidade de decisão nem de operação humana.

Quase todos os domínios da vida, da aplicação de leis à concessão de crédito, da medicina à educação, da agricultura à indústria pesada, podem se beneficiar de Machine Learning. 

Basta haver padrões para que os algoritmos possam aprendê-los e fazer previsões a partir deles. 

Por padrões, entenda-se algo que se repete mais ou menos de forma similar ao longo do tempo. A falta completa de padronização impossibilita ou dificulta muito o trabalho em aprendizado de máquina.

# Machine Learning, Deep Learning e Inteligência Artificial
Representação do relacionamento entre ML, DL e AI e Data Science. Fonte: deviq.io

É comum haver confusão e ambiguidade entre o que é ML, o que é Deep Learning (DL) ou Aprendizado Profundo e Inteligência Artificial (AI, de Artificial Intelligence, em inglês).

Você pode pensar neles como matrioskas, aquelas bonecas russas de vários tamanhos que cabem umas dentro de outras — como os círculos amarelos da imagem acima.

Assim, Deep Learning é um subcampo de Machine Learning, que, por sua vez, é um subcampo da Inteligência Artificial. Ou seja, tudo o que é Deep Learning é, também, Machine Learning. Mas nem todo Machine Learning é Deep Learning. Bem como nem toda a AI é Machine Learning.

Data Science, por sua vez, também se relaciona e usa técnicas de todas essas áreas, mas possui práticas e princípios que fogem a tais domínios. Ou seja, Ciência de Dados não é sinônimo de Machine Learning.

# Artificial Intelligence (AI)
Inteligência Artificial é um domínio amplo, nascido na década de 1950, a partir de pesquisas multidisciplinares em áreas que vão da Psicologia Cognitiva à Linguística, da Computação à Teoria de Sistemas, da Lógica à Estatística.

O campo surgiu revestido de uma certa “magia” desde o início, por se voltar à busca da chamada Artificial General Intelligence (AGI) ou Inteligência Artificial Geral ou, ainda, “IA Forte”, algo capaz de imitar ou até superar a cognição humana — alvo de intensos debates filosóficos, além de material farto ao cinema e à literatura.

Por várias décadas, até o fim dos anos 1980, o campo foi dominado por abordagens lógico-simbólicas, acreditando-se que bastaria fornecer regras lógicas e dicionários de palavras e sentenças aos computadores para que, em pouco tempo, eles conseguissem conversar conosco.

Obviamente, o desafio era bem mais complexo e até hoje não sabemos se estamos próximos ou distantes de atingir a tão almejada AGI.

# Machine Learning (ML)
ML, por sua vez, surgiu de ambições mais modestas, como resolver problemas reais de indústrias, do comércio, de saúde ou para aplicações militares.

O campo não tem um início único, mas surgiu concomitantemente em diversas áreas relacionadas à computação, sobretudo:

* Estatística, que a partir dos anos 1960 passou a fazer uso intensivo de computadores para aprimorar modelos estatístico-matemáticos, e;
* Data Mining (Mineração de Dados), área preocupada no reconhecimento de padrões em grandes bases de dados.

O desenvolvimento dessas áreas levou ao aprimoramento contínuo de métodos e tecnologias, que se revelaram muito úteis principalmente com a chegada da Internet comercial e o surgimento de Big Data (imensas bases de dados que geramos a partir do uso de sites e aplicativos).

Machine Learning “clássico” ou tradicional abrange principalmente métodos estatísticos, desde Regressões Lineares (ajuste de pontos a uma reta) até o Teorema de Bayes, da estatística bayesiana (probabilidade de um evento ocorrer dado o conhecimento a priori que se tem dele), em sua essência.

# Deep Learning (DL)
Deep Learning é subcampo de ML que lida especificamente com redes neurais, algoritmos ou conjuntos de algoritmos com muitos “neurônios artificiais”, capazes tanto de reconhecer padrões quanto de produzir padrões novos, similares aos aprendidos.

A área ganhou holofotes principalmente a partir de 2012 e vive um boom atualmente, embora redes neurais simples, como o perceptron, existam desde os anos 1950.

Muitos dos algoritmos de DL aprendem sozinhos, por tentativa e erros, a partir dos dados que recebem, sem ter de ser treinados de antemão com dados similares.

Isso permite que tal tecnologia aprenda a jogar jogos com rapidez impressionante, “compreenda” e “escreva” textos (e até código de computador) complexos e crie novas imagens realistas que jamais existiram, apenas com base em dados de aprendizado.

Por outro lado, algoritmos de DL necessitam de enormes quantidades de dados e grande poder computacional para serem treinados, o que é caro e viável apenas a grandes empresas de tecnologia, como Google e Microsoft e seus braços de IA, como Deep Mind e Open AI.

# Métodos de aprendizado
Ilustração que exemplifica diferenças entre os métodos não supervisionado e supervisionado.
Fonte: www.extrahop.com 

Há algumas maneiras diferentes dos algoritmos de ML (e aqui englobamos DL também) aprenderem. Vamos ver.

# Aprendizado supervisionado
Como o nome diz, aprendizado supervisionado é quando um algoritmo precisa de algum tipo de supervisão para aprender, mais ou menos como uma criança que depende de um tutor.

Lembre-se do nosso exemplo, lá do início, de classificação de imagens de cães e gatos. 

No aprendizado supervisionado, o algoritmo depende de nós. Precisamos mostrar exemplos de imagens de cães e gatos a ele e ensiná-lo, explicitamente: “esta foto é de um gato”, “está é de um cachorro”, até ele aprender.

Chamamos isto de enviar dados rotulados (muitas fotos classificadas previamente como sendo de gatos ou de cachorros, em nosso exemplo) ao algoritmo, para que ele tenha uma base na qual aprender. 

Só após o treinamento nesses dados rotulados (e testes de precisão, entre outras validações), o algoritmo está pronto para classificar imagens outras que nunca viu antes.

Métodos simples e tradicionais de Machine Learning, baseados em estatística e existentes há mais tempo, costumam adotar esse método.

Uma vantagem é que ele permite usar algoritmos mais simples. A principal desvantagem é, obviamente, não termos dados rotulados para tudo e ser caro rotular determinados tipos de dados do mundo real para treinar modelos.

# Aprendizado não supervisionado
Entendendo o aprendizado supervisionado, fica mais fácil compreender o não supervisionado. 

Algoritmos desse tipo não têm dados rotulados em que se basear. Eles simplesmente processam conjuntos de dados e, por meio de cálculos estatísticos mais sofisticados, tentam reconhecer padrões (distância entre pontos, densidades de probabilidade etc.) nestes conjuntos. 

É a partir do reconhecimento desses padrões que o algoritmo cria um modelo, por meio do qual passa a entender novos dados que recebe.

Caso tenham a capacidade de se autocorrigir, algoritmos desse tipo podem gerar modelos que ficam melhores muito rapidamente, à medida que recebem mais e mais dados.

Algoritmos de Deep Learning (redes neurais) do tipo generativo, que criam novas imagens (como deep fakes, por exemplo) ou novos textos a partir do que aprenderam — um dos hypes da atualidade —, normalmente adotam esse método.

Abordagens mais antigas de mineração de dados também usam e foram pioneiros na abordagem.

A vantagem do método é em domínios onde uma aplicação tem de aprender com o mundo real, onde não há dados rotulados, como em robótica, carros autônomos ou processamento de linguagem natural.

A desvantagem é necessitar de grandes quantidades de dados e grande poder computacional para serem treinados.

# Aprendizado por reforço
Aprendizado por reforço é como que um paradigma diferente de aprendizado. Algoritmos desse tipo possuem mecanismos de punição e recompensa que fazem com que corrijam seu próprio comportamento durante o aprendizado.

Em outras palavras, a abordagem permite que o algoritmo gere um modelo que melhora constantemente, em busca de uma situação ótima (à beira da “perfeição”, digamos).

O aprendizado por reforço pode trabalhar em conjunto com o aprendizado supervisionado ou o não supervisionado — tem sido cada vez mais comum neste último. 

O método também costuma ser associado ao aprendizado semi-supervisionado, em que o algoritmo é submetido a uma parte de dados rotulados, mas também aprende por reconhecimento de padrões em dados não rotulados.

# Algoritmo vs. modelo de ML
Representação divertida e, ao mesmo tempo, instrutiva, sobre a diferença entre dados, algoritmos, modelos e previsões. Fonte: hackernoon.com

Há mais ambiguidades em jogo em ML. Por que ora falamos de algoritmo de ML e ora falamos de modelo de ML? A imagem bem humorada, acima, ajuda a entender de maneira intuitiva.

Uma solução de ML — um mecanismo que faz recomendações de produtos ou conteúdos, um chatbot que conversa com clientes, um classificador que diz se um cliente em potencial é um bom ou mau pagador etc. — é composta por quatro princípios básicos: dados, algoritmos, modelos e previsões.

Podemos pensar neles por analogia a uma pizzaria, como na imagem:

* Dados: são nossa matéria-prima, isto é, os “ingredientes” para fazermos pizzas. A analogia é praticamente perfeita: sem ingredientes, sem pizza (sem dados, sem previsões). E quanto melhor os ingredientes, mais saborosa a pizza. Isto é, a qualidade dos dados é crucial para a qualidade da previsão. Dados “sujos”, “ruins”, mal capturados e medidos: pizza ruim, talvez até estragada.
* Algoritmos: são máquinas, nada mais do que isso. Pense neles como o forno da pizzaria ou uma batedeira para fazer a massa. São máquinas brutas, que processam os ingredientes (dados). Algoritmos são por onde os dados passam para serem calculados e transformados, até se transformarem em uma previsão (pizza). No entanto, algoritmos sozinhos não são a “estrela” da pizzaria. Jogue todos os ingredientes de uma vez — trigo, fermento, água, sal, óleo, mais o queijo, o tomate ou a calabresa — no algoritmo e no máximo sairá algo estranho, mas não uma pizza saborosa.
* Modelo: aqui está a “estrela” de ML. Modelos são a receita para fazer a pizza. Pode haver diversas receitas de pizza, que podem ser assadas no mesmo forno (algoritmo). Cada um gerará previsões melhores ou piores. De outro modo, modelo é o que emerge da combinação de dados e algoritmos: é o aprendizado a partir de ambos. Se tivermos dados padronizados e algoritmos funcionando corretamente, podemos produzir pizzas em série (escala industrial) com poucos problemas. Se tivermos dados que mudam o tempo todo, que exijam um tempo de cozimento maior ou menor a cada vez, podemos ter dificuldades e precisar ajustar a receita (calibrar o modelo) com frequência. 
* Previsão: é a pizza ou o “resultado” de um processo de ML. Podemos pensar na previsão como sendo informações (um índice, um número, um conjunto de percepções) que ajudam humanos a tomarem decisões ou como decisões automatizadas, como no caso dos sistemas de recomendação — em que a máquina, com base nas preferências do cliente, indica conteúdos parecidos com aqueles que ele já consumiu ou gosta, sem precisar de intervenção humana.

# Particularidades
Machine Learning é um campo vasto em conceitos e técnicas, e que se expande mais a cada dia. 

Não queira dominar tudo de uma vez; vá aos poucos, leia, certifique-se que entendeu. Caso não tenha entendido, volte ao assunto após um tempo. Treine, repita exemplos. Repita o processo. O aprendizado virá com o tempo.

À medida que você se aprofunda, alguns conceitos que podem parecer mais complexos se tornarão mais intuitivos.

Por exemplo, você verá menções a termos como backpropagation (retropropagação) ou gradient descent (“descida do gradiente”, em tradução livre). 

Não se assuste. Backpropagation é apenas uma técnica já embutida em redes neurais. Gradient descent é uma técnica de otimização (vem da matemática) presente no núcleo de muitos algoritmos.

Você também irá se deparar com matriz e confusão, acurácia e precisão do modelo, recall e F1. Todas estas são “métricas” ou maneiras, grosso modo, para medir o desempenho de modelos, se eles estão retornando previsões corretas ou enviesadas.

Outros dois conceitos importantes são: treinamento e teste. Em ML, conjuntos de dados costumam ser divididos dessa forma. Uma parte (percentual) dos dados vai para o treinamento do modelo. Outra parte vai para teste, ou seja, para certificar se o modelo foi treinado com eficácia.

Path Mind, uma empresa que fornece soluções de ML a outras empresas, possui uma Wiki que explica mais sobre todos esses conceitos e termos, em linguagem acessível. Vale consumi-los com tempo.

À medida que você se familiariza com a área e quer ver como as ferramentas funcionam ou se aprofundar em alguma delas, essa relação de frameworks Python para ML pode ser bastante útil.

Você não vai precisar conhecer todos os frameworks, é claro, muito menos usará boa parte deles, dependendo de onde atuar. Ter uma noção geral, entretanto, é útil para conhecer as soluções disponíveis. Assim, se houver necessidade, você sabe onde pode se aprofundar para resolver problemas.

Após ter um contato com os conceitos fundamentais e uma noção, por alto, da parte técnica, procure inteirar-se mais sobre ética no uso de ML, vieses e preconceitos de modelos e como tais soluções podem escalar problemas sociais, por exemplo. É um assunto em alta na tecnologia.

# Referências
https://wiki.pathmind.com/
https://en.wikipedia.org/
https://www.ibm.com/
https://hackernoon.com/
https://medium.com/
