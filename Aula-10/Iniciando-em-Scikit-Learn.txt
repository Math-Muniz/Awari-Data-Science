# Leitura: Iniciando em Scikit-Learn
 Logotipo do scikit-learn. Fonte: scikit-learn.org

Scikit-learn é uma biblioteca open source e gratuita para Machine Learning (Aprendizado de Máquina) em Python, que oferece suporte a aprendizado supervisionado e não supervisionado. 

É considerada uma das soluções mais versáteis e populares do mercado. Fornece seleção de recursos (variáveis) eficientes para aprendizado de máquina, modelagem estatística, análise e mineração de dados.

Conta com ferramentas para ajuste, seleção e avaliação de modelos, assim como pré-processamento de dados.

É construída com base em outras bibliotecas Python populares em tarefas numéricas e científicas, como NumPy, SciPy e Matplotlib.

# Principais utilidades
Scikit-learn oferece uma ampla variedade de algoritmos integrados, utilizados em tarefas de:

* Classificação: algoritmos de classificação identificam a categoria associada aos dados fornecidos. Podem servir, por exemplo, para categorizar mensagens de e-mail como spam ou não.
* Regressão: envolve a criação de um modelo que tenta compreender a relação entre os dados de entrada e saída. Algoritmos de regressão podem ser usados, por exemplo, para descrever o comportamento de preços de ações.
* Clusterização (ou agrupamento): algoritmos deste tipo são geralmente utilizados para agrupar automaticamente dados com as mesmas características em conjuntos. Um exemplo seriam os dados de clientes que podem ser segmentados com base em uma determinada faixa-etária ou localização.
* Redução de dimensionalidade: reduz o número de variáveis ​​aleatórias para análise. Por exemplo, para aumentar a eficiência das visualizações, conjuntos de dados com muitas dimensões (grandes  conjuntos de imagens em vídeo, por exemplo) podem ser tratados como se tivessem apenas duas ou três dimensões.
* Seleção de modelo: os algoritmos de seleção de modelos oferecem ferramentas para comparar, validar e selecionar os melhores parâmetros e modelos a serem usados ​​em projetos.
* Pré-processamento: ferramentas de pré-processamento do scikit-learn são importantes na extração e normalização de recursos durante a análise de dados. No contexto de Data Science, é possível usar tais ferramentas para transformar dados de entrada, como texto, e aplicar seus recursos na análise.

# Vantagens de uso
À medida que a popularidade dos algoritmos de Machine Learning aumenta, cresce também a exigência por ferramentas eficientes e versáteis. 

Scikit-learn atende à necessidade de iniciantes na área, bem como aqueles que resolvem problemas de aprendizado supervisionado ou não supervisionado de forma profissional, no mercado. 

Algumas das vantagens da ferramenta são:

* Gratuidade: como scikit-learn é distribuído sob licença BSD (código aberto), é gratuito e pode ser usado por qualquer pessoa.
* Usabilidade: fácil de usar, o scikit-learn tem sido alternativa para muitas empresas e organizações que o utilizam para executar uma variedade de processos. 
* Versatilidade: é uma ferramenta que permite realizar uma infinidade de tarefas, desde criar neuroimagens, identificar ações abusivas na web, até prever o comportamento do consumidor. Não à toa, é utilizada por equipes comerciais e de pesquisa em todo o mundo.
* Apoiado por uma comunidade internacional: desde a sua origem, em 2007, scikit-learn é apoiado por uma comunidade online internacional com a qual os usuários podem contar se tiverem problemas ou dúvidas e para a própria evolução e manutenção da biblioteca.
* Documentação robusta: para ajudar a garantir que usuários novos e antigos obtenham a assistência necessária com relação à integração do scikit-learn com suas plataformas, é fornecida uma extensa e detalhada documentação de seus recursos.

# Exemplos em código
Vamos fazer algumas demonstrações de como alguns algoritmos de scikit-learn funcionam e de como são construídos em código. 

Usaremos o Google Colab, ambiente interativo para Data Science em Python, para as demonstrações.

## Tarefa
Nosso exemplo envolverá uma tarefa de classificação de imagens de flores, com base no Íris, um conjunto de dados muito popular para estudos em Machine Learning, criado em 1936 pelo biólogo e um dos grandes estatísticos da história, Ronald Fischer.

O conjunto reúne informações sobre 150 instâncias (observações ou exemplares) de três variedades (classes) da flor Íris: 

* Iris Setosa;
* Iris Versicolor;
* Iris Virginica.
Os atributos que permitem classificar ou “prever” cada instância ou indivíduo do conjunto em uma dessas três classes (variedades) são:

* comprimento da sépala da flor em centímetros (cm);
* largura da sépala em cm;
* comprimento da pétala em cm;
* largura da pétala em cm.

# Usando Decision Tree
Neste primeiro exemplo, vamos usar o algoritmo de Árvore de Decisão (Decision Tree) do scikit-learn para demonstração.

Este é um dos algoritmos mais simples de entender. É como se ele percorre-se, literalmente, uma árvore de vários “se → então → senão” (decisões). 

Grosso modo, se uma flor tem determinadas características (medidas), então, pertence a uma classe. Senão, pertence a outra. Se pertencer a essa outra e tiver outras determinadas características, então pertence a uma terceira classe. E assim por diante.

O exemplo completo, com modificações, foi inspirado neste e neste tutoriais (em inglês). Vamos ao código.

# Criando um novo arquivo no Colab
Primeiro, acesse o Google Colab e crie um novo arquivo.  Você pode salvá-lo com o nome que quiser. Basta renomeá-lo.

A tarefa é bastante simples, mas a Ajuda do Colab pode ser útil se você tiver dificuldades.

Daqui em diante, iremos nos referir a esse arquivo como notebook, nome popular para “cadernos” de código Python em Data Science.

# Instalando o scikit-learn
A primeira providência para usar o scikit-learn é instalá-lo em nosso notebook. 

Para isso, na primeira célula do arquivo, digite e execute (tecle CTRL Enter ou clique no botão de play da célula para isso):

!pip install scikit-learn
Um log de algumas linhas irá aparecer abaixo da célula. Aguarde a instalação terminar. Você irá automaticamente para uma nova célula.

# Importando as dependências
Precisaremos importar uma série de dependências para nosso código funcionar, desde a biblioteca Pandas até o algoritmo Decision Tree propriamente dito do scikit-learn.

Para isso, digite (ou copie e cole) e execute em outra célula:

# Importando as dependências
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn import metrics
Entendendo o que aconteceu:

importamos Pandas, para manipular os dados;
do scikit-learn, importamos:
utilitário para dividir rapidamente um conjunto de dados nos subconjuntos de treinamento (train) e teste (test), procedimento necessário em Machine Learning;
algoritmo Decision Tree e o método plot_tree, que plota a árvore de decisões de forma gráfica;
utilitário de métricas, para avaliação do modelo.
Obtendo os dados
Obtemos os dados com o seguinte código. Copie e cole-o ou digite isto em uma nova célula. Não esqueça, sempre, de executá-la com CTRL Enter ou com o botão de play da célula.

# Fonte dos dados
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"

# Rótulos que daremos aos atributos (colunas) da nossa tabela de dados
columns = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'variety']

# Criando um DataFrame em Pandas para nosso trabalho
data = pd.read_csv(url, names=columns)
O que aconteceu aqui foi:

atribuímos a fonte do nosso conjunto de dados (o dataset Íris, em formato .csv ou texto separado por vírgula) à variável url;
atribuímos os nomes que queremos dar às colunas (recursos ou variáveis) de nossa tabela de dados à variável columns;
criamos um DataFrame em Pandas usando a fonte de dados e os nomes das colunas.
Analisando rapidamente os dados
Podemos ver os rótulos das colunas do conjunto rodando o método head() de Pandas em uma nova célula:

# Entendendo o cabeçalho do DataFrame
data.head(5)
O resultado deverá ser:


Também podemos ver a quantidade de instâncias de cada variedade da flor Íris. Para isso, rodamos em outra célula:

# Visualizando o número de instâncias por classes
data.groupby('variety').size()
O resultado será:

variety
Iris-setosa        50
Iris-versicolor    50
Iris-virginica     50
dtype: int64
Ótimo! Entendemos um pouco sobre como é o nosso conjunto de dados.

Se você quiser ampliar a análise exploratória do conjunto, pode tentar replicar algumas visualizações desde tutorial, que estamos utilizando como base. 

Apenas atente-se que, para isso, é necessário importar mais dependências e satisfazer outros detalhes, conforme o próprio tutorial. Do contrário, o código não funcionará.

# Criando o classificador
Vamos criar nosso classificador usando o algoritmo Decision Tree (Árvore de Decisão).

Podemos entender mais a respeito do algoritmo e seus parâmetros na documentação do scikit-learn.

Vamos usar uma profundidade (max_depth) de 3 níveis em nossa árvore. Pense nisso como se fossem três camadas de decisão que o algoritmo terá de tomar para classificar nossos dados.

O código ficará assim (mais uma vez, reproduza-o e execute-o em uma nova célula do Colab):

# Criando subconjuntos de treino (train) e teste (test)
train, test = train_test_split(data, test_size = 0.4, stratify = data['variety'], random_state = 42)

# Rotulando os conjuntos de treino e teste
X_train = train[['sepal-length','sepal-width','petal-length','petal-width']]
y_train = train.variety
X_test = test[['sepal-length','sepal-width','petal-length','petal-width']]
y_test = test.variety
No primeiro bloco, criamos os subconjuntos de treino e teste. 

Para teste, usamos 40% (test_size = 0.4) do nosso conjunto de dados original, para garantia de maior precisão, já que se trata de um conjunto pequeno (150 instâncias, apenas). O padrão em ML é usar de 20% a 30% dos dados para testes. 

No segundo bloco, rotulamos e separamos nossos recursos, isto é, nossas variáveis. 

X_train e Y_train são nossas variáveis independentes; x_train e y_train são as variáveis dependentes em cada subconjunto — esses termos, variáveis independentes e independentes, vêm da Estatística.

# Rodando o modelo e medindo a acurácia
Para saber como nosso modelo performa na classificação das variedades da flor Íris, vamos executá-lo e capturar sua métrica de acurácia. 

Em uma nova célula, reproduza o seguinte código para isso:

# Rodando o modelo
mod_dt = DecisionTreeClassifier(max_depth = 3, random_state = 1)
mod_dt.fit(X_train,y_train)
prediction=mod_dt.predict(X_test)

# Medindo a acurácia
print("Acurácia do modelo:","{:.3f}".format(metrics.accuracy_score(prediction,y_test)))
A saída será:

Acurácia do modelo: 0.983
Analisando o resultado

Tivemos uma acurácia de 0.983 ou de 98,3% do nosso modelo. 

Isto quer dizer que em 98,3% dos casos, a partir do conjunto de dados, o modelo soube classificar corretamente, com base nas características (medidas), se uma flor Íris é da variedade Setosa, da Versicolor ou da Virginica.

Nada mal, aparentemente, não?

Para uma simples demonstração, nada mal. Porém, em se tratando de um conjunto de dados com apenas 150 instâncias, pode-se buscar uma precisão muito maior. Como fazer isso?

Testando outros algoritmos e comparando a acurácia entre eles. Vamos exemplificar mais sobre isso logo a seguir.

Visualizando a árvore de decisões
Quer entender, visualmente, como o algoritmo fez a classificação? Temos como desenhar isso com ajuda do scikit-learn.

Em uma nova célula do Colab, digite e execute:

# Visualizando a “árvore” de decisões
variety = ['Setosa', 'Versicolor', 'Virginica']
plt.figure(figsize = (10,8))
plot_tree(mod_dt, feature_names = columns, class_names = variety, filled = True);
O resultado deverá ser:


Basicamente, o algoritmo partiu de determinadas características (transformadas em regras) e decidiu, com base nelas, se a planta era da variedade Setosa. O que não era, ele classificou, a priori, como Versicolor.

Foi, então, para o próximo nível de decisão, decidindo quais eram realmente da variedade Versicolor. As que não se encaixavam nas características, então, só podiam ser Virginicas.

Algumas considerações a mais:

Se você rodar mod_dt.feature_importances_ em uma nova célula, verá uma lista Python, que começam com dois zeros. Isso quer dizer que os tamanhos das sépalas não foram considerados nas decisões, apenas o das pétalas foram suficientes.
Se a largura da pétala é <= 0.7, é Setosa. Se, além disso, o comprimento é <= 4,95, pode ser Versicolor ou Virginica. A partir desse comprimento e com novas larguras de pétalas, fica mais fácil decidir se uma planta é Versicolor ou Virginica, como podemos ver na visualização da árvore.
As caixas em roxo claro é onde pode haver mais incerteza, com base no índice Gini ou medida de impureza dos dados, por isso não temos uma acurácia de 100%.
Seleção de modelos
Uma prática “padrão” em Ciência de Dados e Machine Learning não é se ater apenas a um modelo, como Decision Tree, mas testar os dados em diferentes modelos e comparar quais performam melhor, isto é, executam a tarefa com mais acurácia.

Este tutorial (em inglês), que também usa o conjunto Íris, é um exemplo. Nele, são comparados o desempenho dos modelos gerados pelos algoritmos:

Logistic Regression (LR) ou “Regressão Logística”;
Linear Discriminant Analysis (LDA), ou “Análise Linear Discriminante”, em tradução livre;
K-Nearest Neighbor (KNN), algo como “K-vizinhos mais próximos”;
Decision Tree (DT) ou “Árvore de Decisão”, que acabamos de usar;
Gaussian Naïve Bayes (GNB) ou “Bayes Ingênuo Gaussiano”, em tradução livre, fundamentado no Teorema de Bayes;
Support Vector Classification (SVC), ou “Classificação de Vetores de Suporte”, em tradução livre.
Código da seleção
Implementamos o código abaixo com modificações a partir do tutorial base. Por isso, pode haver leves diferenças entre nosso código e o da referência.

Não vamos entrar em todos os detalhes do código, que é mais complexo, mas sugerimos que você tente reproduzi-lo e estudá-lo.

# Importando as dependências
from pandas import read_csv
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Carregando o conjunto de dados
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"
names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']
dataset = read_csv(url, names=names)

# Criando subconjuntos de treino e teste
array = dataset.values
X = array[:,0:4]
y = array[:,4]
X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)

# Verificação pontual dos algoritmos
models = []
models.append(('Logistic Regression (LR)', LogisticRegression(solver='liblinear', multi_class='ovr')))
models.append(('Linear Discriminant Analysis (LDA)', LinearDiscriminantAnalysis()))
models.append(('K-Nearest Neighbor (KNN)', KNeighborsClassifier()))
models.append(('Decision Tree (DT)', DecisionTreeClassifier()))
models.append(('Gausian Naïve Bayes (GNB)', GaussianNB()))
models.append(('Support Vector Classification (SVC)', SVC(gamma='auto')))

# Avaliando um modelo de cada vez
results = []
names = []
for name, model in models:
kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)
cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
results.append(cv_results)
names.append(name)
print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))

# Comparando o desempenho
pyplot.boxplot(results, labels=["LR", "LDA", "KNN", "DT", "GNB", "SVC"])
pyplot.title('Algorithm Comparison')
pyplot.show()
O resultado da comparação será:

Logistic Regression (LR): 0.941667 (0.065085)
Linear Discriminant Analysis (LDA): 0.975000 (0.038188)
K-Nearest Neighbor (KNN): 0.958333 (0.041667)
Decision Tree (DT): 0.941667 (0.038188)
Gaussian Naïve Bayes (GNB): 0.950000 (0.055277)
Support Vector Classification (SVC): 0.983333 (0.033333)
A partir disso, concluímos que SVC tem a melhor acurácia: 98,34%. 

Previsões
Feita a seleção do modelo, podemos realizar previsões. Em uma nova célula, rodamos:

# Rodando as previsões
model = SVC(gamma='auto')
model.fit(X_train, Y_train)
predictions = model.predict(X_validation)
Avaliação das previsões

Por fim, executamos isto, que nos ajudará com a avaliação das previsões feitas:

# Importando mais dependências
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Avaliando as previsões
print(accuracy_score(Y_validation, predictions))
print(confusion_matrix(Y_validation, predictions))
print(classification_report(Y_validation, predictions))
Receberemos como resultado o seguinte diagnóstico:


A acurácia da previsão é de 0,9667 ou 96,67%. O diagnóstico fornece outras métricas que são úteis em Machine Learning para cada classe (variedade de Iris, no caso), como precisão, recall, f1-score e suporte.

# Conclusão
Esta é uma introdução com exercícios práticos básicos sobre scikit-learn. A biblioteca conta com boa documentação e seu estudo deve ser rotineiro e gradativo para fixação de tudo o que oferece.

Muitos dos conceitos que você verá na documentação são terminologias ou práticas comuns em Data Science e Machine Learning, não exclusivas de sckit-learn. Procure familiarizar-se com os mesmos.

Também procure, sempre, analisar e entender seu código e o que ele realiza. Com a prática, a intuição fica mais fácil e rápida. Por isso treine e repita. Há vários outros tutoriais na web que podem ser replicados com uso de scikit-learn. 

O Kaggle, plataforma mundial de competições em Ciência de Dados e Machine Learning, também tem muitos recursos para aprendizado, experimentos e projetos com a biblioteca, como este e este exemplos.

# Referências
https://pt.wikipedia.org/wiki/
https://scikit-learn.org/stable/
https://opensource.com/article/
https://comparecamp.com/
https://towardsdatascience.com/
https://machinelearningmastery.com/
https://scikit-learn.org/stable/
