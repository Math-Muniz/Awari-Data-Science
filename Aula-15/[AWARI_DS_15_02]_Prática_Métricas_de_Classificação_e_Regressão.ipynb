{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![header-notebooks.png](data:image/png;base64,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)\n",
        "\n",
        "Prática da **Aula 15: Machine Learning V**, do curso de **Data Science** da **[Awari](https://awari.com.br/)**. Para utilizá-la, vá no menu \"Arquivo\" e, em seguida, na opção \"Salvar uma cópia no Drive\". Isto criará uma cópia deste notebook em uma pasta chamada \"Colab Notebooks\", no seu Google Drive pessoal. Use a cópia para criar novas células de código ou executar as células desta prática.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4E6QJ02t86FD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Métricas de Classificação e Regressão**"
      ],
      "metadata": {
        "id": "TBUQuyk2j415"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x5LXjyF5nc7"
      },
      "source": [
        "## **Objetivo**\n",
        "\n",
        "Nesta prática, iremos comentar e demonstrar algumas métricas para modelos de Classificação e Regressão.\n",
        "\n",
        "Após o processo de *feature engineering* (engenharia de recursos ou \"ajuste das variáveis\"), de implementarmos um modelo e obtermos saídas em forma de probabilidades ou classe, a próxima necessidade é entender o quão eficaz o modelo é, baseado em alguma métrica.\n",
        "\n",
        "É importante conhecer métricas e saber como usá-las para saber se os resultados entregues pelo modelo estão sendo satisfatórios ou não.\n",
        "\n",
        "Esta página do [Scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html) fornece uma boa referência sobre o assunto."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prática**"
      ],
      "metadata": {
        "id": "LPFFvzpqk1A0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visão geral**\n",
        "\n",
        "**Métricas de Classificação:**\n",
        "\n",
        "- Accuracy.\n",
        "- Logarithmic Loss.\n",
        "- ROC, AUC.\n",
        "- Confusion Matrix.\n",
        "- Classification Report.\n",
        "\n",
        "**Métricas de Regressão:**\n",
        "\n",
        "- Mean Absolute Error. MAE\n",
        "- Mean Squared Error. MSE\n",
        "- Root Mean Squared Error. RMSE\n",
        "- Root Mean Squared Logarithmic Error. RMSLE\n",
        "- R Square.\n",
        "- Adjusted R Square.\n",
        "\n",
        "Em **problemas de classificação**, usamos dois tipos de algoritmos (dependendo do tipo de saída que ele cria):\n",
        "\n",
        "- **Saída de classe**: Algoritmos como SVM e KNN criam uma saída de classe. Por exemplo, em um problema de classificação binária, as saídas serão 0 ou 1. Os algoritmos do SKLearn/Outros podem converter essas saídas de classe em probabilidade.\n",
        "\n",
        "- **Saída de probabilidade**: Algoritmos como Regressão Logística, Floresta Aleatória, Aumento de Gradiente, Adaboost etc. fornecem saídas de probabilidade. As saídas de probabilidade podem ser convertidas em saídas de classe criando uma probabilidade limite.\n",
        "\n",
        "Em **problemas de regressão**, a saída é sempre contínua por natureza e não requer tratamento adicional."
      ],
      "metadata": {
        "id": "jrDKRjU5k4Ah"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfc9ouiK5nc9"
      },
      "source": [
        "### **Métricas de Classificação**\n",
        "\n",
        "Vamos usar o dataset [diabetes.csv](diabetes.csv) referente a dados deste tipo de doença para nossa prática. Faça upload do mesmo para o seu Google Drive, para poder importá-lo a seguir.\n",
        "\n",
        "A avaliação será feita por `Logistic Regression`, `SGDClassifier` e `RandomForestClassifier`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "G1imlwzSBhE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "6tL29fXpdeWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_data = pd.read_csv('/content/drive/MyDrive/diabetes.csv')\n",
        "\n",
        "X =  diabetes_data.drop([\"Outcome\"],axis = 1)\n",
        "y = diabetes_data[\"Outcome\"]"
      ],
      "metadata": {
        "id": "-AcdxhuYdn5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_data.shape"
      ],
      "metadata": {
        "id": "Vq4ciL8y6WHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_data.head()"
      ],
      "metadata": {
        "id": "Se54VnrS6hHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando vários modelos com vários hiperparâmetros usando o conjunto de treinamento, selecione o modelo e os hiperparâmetros que apresentam melhor desempenho no conjunto de validação.\n",
        "# Uma vez que o tipo de modelo e os hiperparâmetros tenham sido selecionados, treinamos o modelo final usando esses hiperparâmetros no conjunto de treinamento completo, o erro generalizado é finalmente medido no conjunto de teste.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 56)\n"
      ],
      "metadata": {
        "id": "tIeIpwTGdqRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# StratifiedKFold class realiza amostragem estratificada para produzir dobras\"folds\" que contêm uma proporção representativa de cada classe.\n",
        "cv = StratifiedKFold(n_splits=10, shuffle = True, random_state = 76)"
      ],
      "metadata": {
        "id": "u70tbNGzdvs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinando diversos modelos"
      ],
      "metadata": {
        "id": "rVt0y6FAemTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression - Regressão Logística\n",
        "clf_logreg = LogisticRegression()\n",
        "# fit model - treino do modelo\n",
        "clf_logreg.fit(X_train, y_train)\n",
        "# Previsões de classe para o conjunto de validação.\n",
        "y_pred_class_logreg = cross_val_predict(clf_logreg, X_train, y_train, cv = cv)\n",
        "# probabilidades previstas para a classe 1, probabilidades da classe positiva\n",
        "y_pred_prob_logreg = cross_val_predict(clf_logreg, X_train, y_train, cv = cv, method=\"predict_proba\")\n",
        "y_pred_prob_logreg_class1 = y_pred_prob_logreg[:, 1]\n"
      ],
      "metadata": {
        "id": "FQtYOK6wdy9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "clf_rfc = RandomForestClassifier()\n",
        "# fit model - treino\n",
        "clf_rfc.fit(X_train, y_train)\n",
        "# Previsões de classe para o conjunto de validação.\n",
        "y_pred_class_rfc = cross_val_predict(clf_rfc, X_train, y_train, cv = cv)\n",
        "# probabilidades previstas para a classe 1\n",
        "y_pred_prob_rfc = cross_val_predict(clf_rfc, X_train, y_train, cv = cv, method=\"predict_proba\")\n",
        "y_pred_prob_rfc_class1 = y_pred_prob_rfc[:, 1]"
      ],
      "metadata": {
        "id": "ao_ygHrZd2Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD Classifier\n",
        "clf_SGD = SGDClassifier()\n",
        "# fit model - treino\n",
        "clf_SGD.fit(X_train, y_train)\n",
        "# Previsões de classe para o conjunto de validação.\n",
        "y_pred_class_SGD = cross_val_predict(clf_SGD, X_train, y_train, cv = cv)\n",
        "# probabilidades previstas para a classe 1\n",
        "y_pred_prob_SGD = cross_val_predict(clf_SGD, X_train, y_train, cv = cv, method=\"decision_function\")"
      ],
      "metadata": {
        "id": "uN8zajFRdkwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuG0JKIl5nc_"
      },
      "source": [
        "**Nota rápida**: `predict_log_proba` do Scikit-Learn dá o logaritmo das probabilidades. Isso é mais prático porque as probabilidades podem tornar-se muito, muito pequenas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Acurácia nula**\n",
        "\n",
        "- Precisão que poderia ser alcançada prevendo sempre a classe mais frequente.\n",
        "- Isso significa que um modelo \"burro\" que sempre prevê 0/1 estaria certo \"null_accuracy\" % das vezes."
      ],
      "metadata": {
        "id": "jlwO9Yo_qFSa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j9b1F9h5nc_"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "import numpy as np\n",
        "\n",
        "class BaseClassifier(BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        pass\n",
        "    def predict(self, X):\n",
        "        return np.zeros((len(X), 1), dtype=bool)\n",
        "\n",
        "base_clf = BaseClassifier()\n",
        "cross_val_score(base_clf, X_train, y_train, cv=10, scoring=\"accuracy\").mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXkxaYu75ndB"
      },
      "source": [
        "#### **Classification Accuracy** ou **Acurácia**\n",
        "\n",
        "*Classification Accuracy* ou Acurácia é a razão entre o número de previsões corretas e o número total de amostras de entrada.\n",
        "\n",
        "$$Accuracy = \\frac{Number\\ of\\ correct\\ predictions}{Total\\ number\\ of\\ predictions\\ made} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
        "\n",
        "**Quando usar esta métrica?**\n",
        "\n",
        "Quando há um número aproximado de amostras pertencentes a cada classe.\n",
        "\n",
        "**Quando não usar?**\n",
        "\n",
        "Quando apenas uma classe detém a maioria das amostras.\n",
        "\n",
        "**Exemplo:** considere que há 98% de amostras da classe A e 2% das amostras da classe B em nosso conjunto de treinamento. Então nosso modelo pode facilmente obter 98% de precisão de treinamento simplesmente prevendo cada amostra de treinamento pertencente à classe A.\n",
        "\n",
        "Quando o mesmo modelo é testado em um conjunto de teste com 60% de amostras de classe A e 40% de amostras de classe B, a precisão do teste cairia para 60%. A precisão da classificação pode nos dar a falsa sensação de alcançar alta precisão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjQxjCAI5ndB"
      },
      "outputs": [],
      "source": [
        "# calculate accuracy\n",
        "\n",
        "acc_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
        "acc_SGD = cross_val_score(clf_SGD, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
        "acc_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = 'accuracy').mean()\n",
        "\n",
        "acc_logreg, acc_SGD, acc_rfc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y1FJKzQ5ndC"
      },
      "source": [
        "#### **Logarithmic Loss / Log Loss / Logistic Loss / Cross-Entropy Loss** (perda logarítmica)\n",
        "\n",
        "- Ao trabalhar com Log Loss, o classificador deve atribuir probabilidade a cada classe para todas as amostras.\n",
        "- Log loss mede a INCERTEZA das probabilidades do modelo comparando-as com os rótulos verdadeiros e penalizando as classificações falsas.\n",
        "- A perda de log é definida apenas para dois ou mais rótulos.\n",
        "- Log Loss diminui gradualmente à medida que a probabilidade prevista melhora, portanto Log Loss mais próximo de 0 indica maior precisão, Log Loss longe de 0 indica menor precisão.\n",
        "- Log Loss existe na faixa (0, ∞].\n",
        "\n",
        "Suponha que existam N amostras pertencentes a M classes, então o Log Loss é calculado como abaixo:\n",
        "\n",
        "$$ Log\\ Loss = \\frac{-1}{N} \\sum_{i=1}^{N} \\sum_{i=1}^{M}  y_{ij} * \\log(\\hat{y_{ij}})$$\n",
        "\n",
        "Onde,\n",
        "\n",
        "- $y_{ij}$, indica se a amostra i pertence à classe j ou não\n",
        "\n",
        "- $p_{ij}$, indica a probabilidade da amostra i pertencer à classe j\n",
        "\n",
        "\n",
        "O sinal negativo nega a saída $\\log(\\hat{y_{ij}})$ que é sempre negativa. $\\hat{y_{ij}}$ gera uma probabilidade (0 - 1), $\\log(x)$ é negativo se 0 < x < 1.\n",
        "\n",
        "<b>Exemplo </b>: deixe que os rótulos de treinamento sejam 0 e 1, mas nossas previsões de treinamento sejam 0,4, 0,6, 0,89 etc. Para calcular uma medida do erro do nosso modelo, podemos classificar todas as observações com valores > 0,5 em 1 .Mas, ao fazê-lo, corremos um alto risco de aumentar a classificação incorreta. Isso ocorre porque pode acontecer que muitos valores com probabilidades 0,4, 0,45, 0,49 possam ter um valor verdadeiro de 1.\n",
        "\n",
        "É aqui que o logLoss entra em cena.\n",
        "\n",
        "Agora vamos seguir de perto a fórmula de LogLoss. Pode haver 4 casos principais para os valores de $y_{ij}$ e $p_{ij}$\n",
        "\n",
        "- Caso 1: $y_{ij}$=1, $p_{ij}$ = Alto\n",
        "\n",
        "- Caso 2: $y_{ij}$=1, $p_{ij}$ = Baixo\n",
        "\n",
        "- Caso 3: $y_{ij}$=0, $p_{ij}$ = Baixo\n",
        "\n",
        "- Caso 4: $y_{ij}$=0, $p_{ij}$ = Alto\n",
        "\n",
        "**Como o LogLoss mede a incerteza?**\n",
        "\n",
        "Se tivermos mais Caso 1 e Caso 3, então a soma (e média) dentro da fórmula de logloss seria maior e seria substancialmente maior em comparação com o que teria sido se o Caso 2 e o Caso 4 fossem adicionados. Agora esse valor é o maior possível do Caso 1 e do Caso 3, o que indica uma boa previsão. Se o multiplicarmos por (-1) , tornaremos o valor o menor possível. Isso agora significaria intuitivamente - Quanto menor o valor, melhor é o modelo, ou seja, menor o logloss, melhor é o modelo, ou seja, menor a incerteza, melhor é o modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I5ep0K35ndC"
      },
      "outputs": [],
      "source": [
        "# calculate logloss\n",
        "\n",
        "logloss_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()\n",
        "logloss_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()\n",
        "\n",
        "# SGDClassifier's hinge loss doesn't support probability estimates.\n",
        "# We can set SGDClassifier as the base estimator in Scikit-learn's CalibratedClassifierCV, which will generate probability estimates.\n",
        "\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "new_clf_SGD = CalibratedClassifierCV(clf_SGD)\n",
        "new_clf_SGD.fit(X_train, y_train)\n",
        "logloss_SGD = cross_val_score(new_clf_SGD, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()\n",
        "\n",
        "logloss_logreg, logloss_SGD, logloss_rfc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIbWQtlU5ndD"
      },
      "source": [
        "#### **ROC Curve** ou **Curva ROC**\n",
        "\n",
        "ROC pode ser dividido em sensibilidade e especificidade. Escolher o melhor modelo é uma espécie de equilíbrio entre prever 1's com precisão ou 0's com precisão. Em outras palavras, sensibilidade e especificidade.\n",
        "\n",
        "- **True Positive Rate (Sensibilidade/Recall):** A True Positive Rate é definida como TP/ (FN+TP). True Positive Rate corresponde à proporção de pontos de dados positivos que são corretamente considerados positivos, com relação a todos os pontos de dados positivos.\n",
        "\n",
        "- **Taxa de falsos positivos (Especificidade):** A taxa de falsos positivos é definida como FP / (FP+TN). Taxa de falsos positivos corresponde à proporção de pontos de dados negativos que são erroneamente considerados positivos, em relação a todos os pontos de dados negativos.\n",
        "\n",
        "Taxa de verdadeiro positivo e taxa de falso positivo têm valores no intervalo [0, 1]. Ambos TPR e FPR são calculados em valores de limiar como (0,00, 0,02, 0,04, …., 1,00) e um gráfico é desenhado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9KruBtV5ndD"
      },
      "outputs": [],
      "source": [
        "# IMPORTANTE: primeiro argumento são valores verdadeiros, segundo argumento são probabilidades previstas\n",
        "\n",
        "# passamos y_test e y_pred_prob\n",
        "# não usamos y_pred_class, pois dará resultados incorretos sem gerar erro\n",
        "# roc_curve retorna 3 objetos taxa de falso positivo (fpr), taxa de verdadeiro positivo (tpr), limites\n",
        "\n",
        "fpr_logreg, tpr_logreg, thresholds_logreg = metrics.roc_curve(y_train, y_pred_prob_logreg_class1)\n",
        "fpr_rfc, tpr_rfc, thresholds_rfc = metrics.roc_curve(y_train, y_pred_prob_rfc_class1)\n",
        "fpr_SGD, tpr_SGD, thresholds_SGD = metrics.roc_curve(y_train, y_pred_prob_SGD)\n",
        "\n",
        "plt.plot(fpr_logreg, tpr_logreg, label=\"logreg\")\n",
        "plt.plot(fpr_rfc, tpr_rfc, label=\"rfc\")\n",
        "plt.plot(fpr_SGD, tpr_SGD, label=\"SGD\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCnsFtt25ndD"
      },
      "source": [
        "**Interpretando o gráfico ROC**\n",
        "\n",
        "Interpretar o gráfico ROC é muito diferente de um gráfico de linha regular. Porque, embora haja um eixo X e um eixo Y, não o lemos como: para um valor X de 0,25, o valor Y é 0,9.\n",
        "\n",
        "Em vez disso, o que temos aqui é uma linha que traça o corte de probabilidade de 1 no canto inferior esquerdo a 0 no canto superior direito.\n",
        "\n",
        "Essa é uma maneira de analisar como a sensibilidade e a especificidade se comportam para toda a faixa de pontos de corte de probabilidade, ou seja, de 0 a 1.\n",
        "\n",
        "Idealmente, se tivermos um modelo perfeito, todos os eventos terão uma pontuação de probabilidade de 1 e todos os não eventos terão uma pontuação de 0. Para tal modelo, a área sob o ROC será um perfeito 1.\n",
        "\n",
        "Assim, se traçarmos a curva a partir do canto inferior esquerdo, o valor de corte de probabilidade diminui de 1 para 0. Se tivermos um bom modelo, mais eventos reais devem ser previstos como eventos, resultando em alta sensibilidade e baixo FPR. Nesse caso, a curva subirá abruptamente cobrindo uma grande área antes de atingir o canto superior direito.\n",
        "\n",
        "Portanto, quanto maior a área sob a curva ROC, melhor é o modelo.\n",
        "\n",
        "A curva ROC é a única métrica que mede o desempenho do modelo para diferentes valores de pontos de corte de probabilidade de previsão.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttX5gMxz5ndE"
      },
      "outputs": [],
      "source": [
        "# define uma função que aceita um limite(threshold) e imprime sensibilidade e especificidade\n",
        "def evaluate_threshold(tpr, fpr,clf_threshold, threshold):\n",
        "    print('Sensitivity:', tpr[clf_threshold > threshold][-1])\n",
        "    print('Specificity:', 1 - fpr[clf_threshold > threshold][-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avalidando diferentes modelos"
      ],
      "metadata": {
        "id": "RwmTIFPkfmTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iMG662D5ndE"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression - Regressão Logística\n",
        "evaluate_threshold(tpr_logreg, fpr_logreg, thresholds_logreg, 0.2), evaluate_threshold(tpr_logreg, fpr_logreg, thresholds_logreg, 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8PiZb2E5ndF"
      },
      "outputs": [],
      "source": [
        "# Random Forest Classifier - Árvores aleatórias\n",
        "evaluate_threshold(tpr_rfc, fpr_rfc, thresholds_rfc, 0.2), evaluate_threshold(tpr_rfc, fpr_rfc, thresholds_rfc, 0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svneaMhJ5ndF"
      },
      "outputs": [],
      "source": [
        "# SGD - Stochastic gradient descent\n",
        "evaluate_threshold(tpr_SGD, fpr_SGD, thresholds_SGD, 0.2), evaluate_threshold(tpr_SGD, fpr_SGD, thresholds_SGD, 0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Rwga9p5ndF"
      },
      "source": [
        "#### **AUC**\n",
        "- A interpretação probabilística da pontuação ROC-AUC é que se escolhermos aleatoriamente um caso positivo e um caso negativo, a probabilidade de que o caso positivo supere o caso negativo de acordo com o classificador é dada pela AUC. Aqui, a classificação é determinada de acordo com a ordem dos valores previstos.\n",
        "- A pontuação ROC-AUC é independente do limite estabelecido para classificação, pois considera apenas a classificação de cada previsão e não seu valor absoluto. O mesmo não é verdade para a pontuação F1, que precisa de um valor limite em caso de saída de probabilidades\n",
        "- AUC é a porcentagem do gráfico ROC que está abaixo da curva.\n",
        "- A AUC representa a capacidade de um modelo de discriminar entre classes positivas e negativas. Uma área de 1,0 representa um modelo que fez todas as previsões perfeitamente. Uma área de 0,5 representa um modelo tão bom quanto aleatório.\n",
        "- AUC é útil mesmo quando há alto desequilíbrio de classe (ao contrário da precisão da classificação)\n",
        "- Caso de fraude\n",
        "     - Precisão nula quase 99%\n",
        "     - AUC é útil aqui\n",
        "\n",
        "General AUC predictions (predições da curva AUC):\n",
        "- .90-1 = Excellent\n",
        "- .80-.90 = Good\n",
        "- .70-.80 = Fair\n",
        "- .60-.70 = Poor\n",
        "- .50-.60 = Fail\n",
        "\n",
        "AUC ROC considera as probabilidades previstas para determinar o desempenho do modelo. Mas, leva em consideração apenas a ordem das probabilidades e, portanto, não leva em consideração a capacidade do modelo de prever maior probabilidade para amostras com maior probabilidade de serem positivas (Log Loss).\n",
        "\n",
        "Enquanto a AUC é calculada com relação à classificação binária com um limite de decisão variável, a perda de log na verdade leva em consideração a “certeza” da classificação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNA9mMVL5ndG"
      },
      "outputs": [],
      "source": [
        "roc_auc_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = 'roc_auc').mean()\n",
        "roc_auc_SGD = cross_val_score(clf_SGD, X_train, y_train, cv = cv, scoring = 'roc_auc').mean()\n",
        "roc_auc_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = 'roc_auc').mean()\n",
        "\n",
        "roc_auc_logreg, roc_auc_SGD, roc_auc_rfc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj_YmhWj5ndH"
      },
      "source": [
        "####  **Confusion Matrix** ou **Matriz de Confusão**\n",
        "\n",
        "Uma matriz de confusão é uma matriz N X N, onde N é o número de classes que estão sendo previstas. A Matriz de Confusão nos dá uma matriz como saída e descreve o desempenho completo do modelo.\n",
        "\n",
        "As previsões corretas caem na linha diagonal da matriz.\n",
        "\n",
        "4 termos importantes na Matriz de Confusão:\n",
        "- Verdadeiros Positivos: Os casos em que previmos SIM e a saída real também foi SIM.\n",
        "- Verdadeiros Negativos: Os casos em que previmos NÃO e a saída real foi NÃO.\n",
        "- Falsos Positivos: Os casos em que previmos SIM e a saída real foi NÃO.\n",
        "- Falsos Negativos: Os casos em que previmos NÃO e a saída real foi SIM.\n",
        "\n",
        "A matriz Confusion em si não é uma medida de desempenho como tal, mas quase todas as métricas de desempenho são baseadas na Matriz Confusion e nos números dentro dela."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliando os diferentes modelos"
      ],
      "metadata": {
        "id": "8BDyQZHDgHsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA9t8Xac5ndH"
      },
      "outputs": [],
      "source": [
        "logreg_matrix = metrics.confusion_matrix(y_train, y_pred_class_logreg)\n",
        "print(logreg_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF0OnpPo5ndI"
      },
      "outputs": [],
      "source": [
        "SGD_matrix = metrics.confusion_matrix(y_train, y_pred_class_SGD)\n",
        "print(SGD_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_kAqG8k5ndI"
      },
      "outputs": [],
      "source": [
        "rfc_matrix = metrics.confusion_matrix(y_train, y_pred_class_rfc)\n",
        "print(rfc_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhhRMv605ndI"
      },
      "source": [
        "####  Classification Report\n",
        "\n",
        "A função `class_report()` exibe a precisão, recall, f1-score e suporte para cada classe.\n",
        "\n",
        "**Precision**\n",
        "\n",
        "É o número de Verdadeiros Positivos dividido pelo número de resultados positivos previstos pelo classificador.\n",
        "\n",
        "$$ Precision =  \\frac{True\\ Positives}{True\\ Positives + False\\ Positives} $$\n",
        "\n",
        "![Screen%20Shot%202019-10-17%20at%2009.10.10.png](attachment:Screen%20Shot%202019-10-17%20at%2009.10.10.png)\n",
        "\n",
        "**Recall/ Sensitivity**\n",
        "\n",
        "É o número de Verdadeiros Positivos dividido pelo número de todas as amostras relevantes (todas as amostras que deveriam ter sido identificadas como positivas).\n",
        "\n",
        "$$Recall =  \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives}$$\n",
        "\n",
        "![Screen%20Shot%202019-10-17%20at%2009.10.38.png](attachment:Screen%20Shot%202019-10-17%20at%2009.10.38.png)\n",
        "\n",
        "- Para minimizar falsos negativos, gostaríamos que nosso Recall fosse o mais próximo de 100%\n",
        "- Para minimizar falsos positivos, queremos que nossa precisão seja o mais próximo de 100%\n",
        "\n",
        "#### <u> Specificity / TNR (True Negative Rate)</u>\n",
        "\n",
        "- Proporção de casos negativos reais corretamente identificados.\n",
        "- Especificidade é exatamente o oposto de Recall.\n",
        "\n",
        "$$Specificity =  \\frac{True\\ Negatives}{True\\ Negatives + False\\ Positives}$$\n",
        "\n",
        "![Screen%20Shot%202019-10-17%20at%2009.11.10.png](attachment:Screen%20Shot%202019-10-17%20at%2009.11.10.png)\n",
        "\n",
        "**F1 Score**\n",
        "\n",
        "- F1 Score é a média harmônica entre precisão e recall.\n",
        "\n",
        "- Diz quão preciso é o classificador (quantas instâncias ele classifica corretamente), bem como quão robusto ele é (não perde um número significativo de instâncias).\n",
        "- Quanto maior o F1 Score, melhor é o desempenho do nosso modelo.\n",
        "- Faixa [0, 1].\n",
        "\n",
        "$$F1 = 2 * \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}$$\n",
        "\n",
        "**Why Harmonic Mean?**\n",
        "\n",
        "Ex: Temos um modelo de classificação binária com os seguintes resultados:\n",
        "\n",
        "Precisão: 0, Recall: 1\n",
        "\n",
        "Se tomarmos a média aritmética, obtemos 0,5. É claro que o resultado acima vem de um classificador burro que apenas ignora a entrada e apenas prevê uma das classes como saída. Agora, se tomarmos HM, obteremos 0, o que é preciso, pois esse modelo é inútil para todos os fins.\n",
        "\n",
        "A média harmônica é uma média quando x e y são iguais. Mas quando x e y são diferentes, então está mais próximo do número menor em comparação com o número maior. Se um número for muito pequeno entre precisão e recuperação, o F1 Score de levanta uma bandeira e está mais próximo do número menor do que do maior, dando ao modelo uma pontuação apropriada em vez de apenas uma média aritmética."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analisando o Log Report"
      ],
      "metadata": {
        "id": "blVL-73XgQvz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQjI6GEH5ndJ"
      },
      "outputs": [],
      "source": [
        "report_logreg = metrics.classification_report(y_train, y_pred_class_logreg)\n",
        "report_SGD = metrics.classification_report(y_train, y_pred_class_SGD)\n",
        "report_rfc = metrics.classification_report(y_train, y_pred_class_rfc)\n",
        "print(\"report_logreg \" +  \"\\n\" + report_logreg,\"report_SGD \"  +  \"\\n\" +  report_SGD,\"report_rfc \"  +  \"\\n\" +  report_rfc, sep = \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGQc86pv5ndJ"
      },
      "source": [
        "**Precision: Recall Tradeoff**\n",
        "\n",
        "Em alguns contextos nos preocupamos principalmente com a precisão e em outros contextos nos preocupamos com a recordação. Por exemplo, se treinarmos um classificador para detectar vídeos seguros para crianças, ew provavelmente preferiria um classificador que rejeitasse muitos vídeos bons (baixo recall), mas mantenha apenas os seguros (alta precisão). Por outro lado, suponha que treinamos um classificador para detectar ladrões de lojas em imagens de vigilância: provavelmente não há problema se nosso classificador tiver apenas 30% de precisão, desde que tenha 99% de recall (com certeza, os guardas de segurança receberão alguns alertas falsos, mas quase todos os ladrões serão pegos).\n",
        "\n",
        "Aumentar a Precisão reduz o Recall e vice-versa. Isso é chamado de <i> Precisão - Troca de Recall </i>.\n",
        "\n",
        "Para entender essa compensação, vamos ver como o SGDClassifier / LogisticRegression / RandomForestClassifier toma suas decisões de classificação. Para cada instância, eles computam uma pontuação com base em uma função de decisão /predict_proba e, se essa pontuação for maior que um limite, eles atribuem a instância à classe positiva, ou então a atribui à classe negativa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub7tRClh5ndK"
      },
      "source": [
        "Scikit-Learn does not let us set the threshold directly, but it does give us access to the decision scores that it uses to make predictions. Instead of calling the classifier’s predict() method, we can call its decision_function() method, which returns a score for each instance, and then make predictions based on those scores using any threshold we want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMn5lDQ75ndK"
      },
      "outputs": [],
      "source": [
        "y_decision_function_scores = clf_logreg.decision_function(X_train)\n",
        "y_decision_function_scores[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBO_k-TG5ndK"
      },
      "outputs": [],
      "source": [
        "threshold = 0\n",
        "y_decision_function_pred = (y_decision_function_scores[6] > threshold)\n",
        "y_decision_function_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC26DP995ndK"
      },
      "source": [
        "O classificador usa um limite(threshold) igual a 0, portanto, o código anterior retorna o mesmo resultado que o método predict() (ou seja, True). Vamos aumentar o limite para 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmEqH5H95ndL"
      },
      "outputs": [],
      "source": [
        "threshold = 2\n",
        "y_decision_function_pred = (y_decision_function_scores[6] > threshold)\n",
        "y_decision_function_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlJN0sz-5ndL"
      },
      "source": [
        "Isso confirma que aumentar o limite (thresold) diminui o recall.\n",
        "\n",
        "A instância na verdade representa um 1(True) e o classificador o detecta quando o limite é 0, mas o perde quando o limite é aumentado para 2.\n",
        "\n",
        "Para decidir qual limite usar, primeiro precisamos obter as pontuações de todas as instâncias no conjunto de treinamento usando a função cross_val_predict() novamente, mas desta vez especificando que você deseja que ela retorne pontuações/probabilidades de decisão em vez de classe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0zMwKhQ5ndL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train, y_pred_prob_logreg_class1)\n",
        "\n",
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02cDImzA5ndM"
      },
      "source": [
        "Agora podemos simplesmente selecionar o valor limite que nos dá a melhor compensação de precisão/recall para nossa tarefa. vamos supor que você decida buscar 80% de recall.\n",
        "\n",
        "Olhamos o primeiro gráfico ou \"plot\" (aumentando um pouco) e descobrimos que precisamos usar um limite de cerca de 0,32.\n",
        "\n",
        "Para fazer previsões (no conjunto de treinamento por enquanto), em vez de chamar o método predict() do classificador, você pode simplesmente executar este código:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaG-PQgN5ndM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "y_pred_90 = (y_pred_prob_logreg_class1 > 0.32)\n",
        "\n",
        "precisionScore = precision_score(y_train, y_pred_90)\n",
        "recallScore = recall_score(y_train, y_pred_90)\n",
        "precisionScore, recallScore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_aBqdJm5ndM"
      },
      "source": [
        "#### **Conclusão**\n",
        "\n",
        "##### **Comparação de Log-loss com ROC & F1**\n",
        "\n",
        "**Caso 1: Conjunto de dados balanceado**\n",
        "\n",
        "| S.No. | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| Actual (Balanced) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
        "| Predicted (Model 1) | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.6 | 0.6 | 0.5 | 0.5 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 |\n",
        "| Predicted (Model 1) | 0.6 | 0.6 | 0.6 | 0.6 | 0.6 | 0.6 | 0.6 | 0.6 | 0.7 | 0.7 | 0.7 | 0.7 | 0.8 | 0.8 | 0.8 | 0.8 |\n",
        "\n",
        "Considere o caso 1 (dados balanceados), parece que o modelo 1 está fazendo um trabalho melhor na previsão das probabilidades absolutas, enquanto o modelo 2 está funcionando melhor na classificação das observações de acordo com seus rótulos verdadeiros. Vamos verificar com a pontuação real:\n",
        "\n",
        "| F1 (threshold = 0.5) | F1 (threshold which maximize score) | ROC - AUC | LogLoss |\n",
        "| --- | --- | --- | --- |\n",
        "| Model 1 | 0.88 | .88 | 0.94 | 0.28\n",
        "| Model 2 | 0.67 | 1 | 1 | 0.6\n",
        "\n",
        "Se considerarmos o log loss, o Modelo 2 é o pior, dando um alto valor de perda de log porque as probabilidades absolutas têm grande diferença dos rótulos reais. Mas isso está em total desacordo com a pontuação F1 e AUC, segundo a qual o Modelo 2 tem 100% de precisão. Além disso, gostaríamos de observar que, com diferentes limites, a pontuação F1 está mudando e preferindo o modelo 1 ao modelo 2 para o limite padrão de 0,5.\n",
        "\n",
        "<b> Inferências extraídas do exemplo acima (conjunto de dados balanceado) </b>:\n",
        "- Se nos importamos com a diferença probabilística absoluta, vamos com Log Loss\n",
        "- Se nos importamos apenas com a previsão da classe final e não queremos ajustar o limite, vá com a pontuação AUC.\n",
        "-A pontuação F1 é sensível ao limiar e gostaríamos de ajustá-la primeiro antes de comparar os modelos.\n",
        "\n",
        "##### **Caso 2: Conjunto de dados desbalanceado**\n",
        "\n",
        "**A) Desbalanceado - Poucos Positivos**\n",
        "\n",
        "| S.No. | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| Actual (Balanced) | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |\n",
        "| Predicted (Model 1) | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.9 | 0.9 |\n",
        "| Predicted (Model 1) | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.9 | 0.9 | 0.9 | 0.9 |\n",
        "\n",
        "| | F1 (threshold = 0.5) | ROC - AUC | LogLoss |\n",
        "| --- | --- | --- | --- |\n",
        "| Model 1 | 0.8 | .83 | .24\n",
        "| Model 2 | 0.86 | .96 | .24\n",
        "\n",
        "A única diferença no modelo1 e no modelo2 é sua previsão para a observação 13 e 14. O modelo 1 está fazendo um trabalho melhor na classificação da observação 13 (rótulo 0), enquanto o modelo 2 está se saindo melhor na classificação da observação 14 (rótulo 1). O objetivo é ver qual modelo realmente captura melhor a diferença na classificação da classe desequilibrada (classe com poucas observações, aqui está o rótulo 1). Em problemas como detecção de fraude/detecção de spam, onde os rótulos positivos são poucos, gostaríamos que nosso modelo predissesse classes positivas corretamente e, portanto, às vezes preferimos aqueles modelos que são capazes de classificar esses rótulos positivos.\n",
        "\n",
        "Claramente, a perda de log está falhando neste caso porque, de acordo com a perda de log, ambos os modelos estão funcionando igualmente. Isso ocorre porque a função log-loss é simétrica e não diferencia entre classes.\n",
        "\n",
        "Tanto a pontuação F1 quanto a pontuação ROC-AUC estão se saindo melhor ao preferir o modelo 2 ao modelo 1. Portanto, podemos usar esses dois métodos para desequilíbrio de classe.\n",
        "\n",
        "**B) Desbalanceado - Poucos Negativos**\n",
        "\n",
        "| S.No. | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| Actual (Balanced) | 0 | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |\n",
        "| Predicted (Model 1) | 0.1 | 0.1 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 |\n",
        "| Predicted (Model 1) | 0.1 | 0.1 | 0.1 | 0.1 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 |\n",
        "\n",
        "|| F1 (threshold = 0.5) | ROC - AUC | LogLoss |\n",
        "| --- | --- | --- | --- |\n",
        "| Model 1 | 0.963 | .83 | .24\n",
        "| Model 2 | 0.96 | .96 | .24\n",
        "\n",
        "A pontuação ROC-AUC tratou o caso de poucos rótulos negativos da mesma forma que tratou o caso de poucos rótulos positivos. A pontuação F1 é praticamente a mesma para o Modelo 1 e o Modelo 2 porque os rótulos positivos são grandes em número e se preocupa apenas com a classificação incorreta dos rótulos positivos.\n",
        "\n",
        "##### **Inferências extraídas do exemplo acima (conjunto de dados desequilibrado)**\n",
        "\n",
        "- Se cuidarmos de uma classe que é menor em número independente do fato de ser positiva ou negativa, vá para a pontuação ROC-AUC.\n",
        "\n",
        "##### **Quando vamos preferir F1 sobre ROC-AUC?**\n",
        "\n",
        "Prefira a curva PR sempre que a classe positiva for rara ou quando nos preocuparmos mais com os falsos positivos do que com os falsos negativos.\n",
        "\n",
        "Para treinar classificadores binários, escolha a métrica apropriada para a tarefa, avalie os classificadores usando validação cruzada, selecione a compensação de precisão/recuperação que atenda às nossas necessidades e compare vários modelos usando curvas ROC e pontuações ROC AUC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw5HBGcU5ndN"
      },
      "source": [
        "### **Métricas de Regressão**\n",
        "\n",
        "Usaremos o dataset [housing.csv](https://drive.google.com/file/d/1jAhlBKt-dKz3kwt2Xav1WuWg58gKWxB-/view?usp=share_link) para esta prática."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjlf59Ry5ndN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "housing_data = pd.read_csv('/content/drive/MyDrive/housing.csv')\n",
        "housing_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X =  housing_data.drop([\"MEDV\"],axis = 1)\n",
        "y = housing_data[\"MEDV\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "# fit model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make class predictions for the testing set\n",
        "y_pred_class = model.predict(X_test)"
      ],
      "metadata": {
        "id": "l4UQY6uyHMjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKo7etJT5ndN"
      },
      "source": [
        "#### **MAE - Mean Absolute Error** ou  **Erro médio absoluto**\n",
        "- Média da diferença entre os Valores Originais e os Valores Previstos.\n",
        "- Não dá nenhuma ideia da direção do erro, ou seja, se estamos prevendo os dados ou superestimando os dados.\n",
        "- Quanto menor o MAE, melhor é o modelo.\n",
        "- Robusto para outliers\n",
        "- Alcance (0, + infinito]\n",
        "\n",
        "$$ Mean\\ Absolute\\ Error = \\frac{1}{N} \\sum_{i=1}^{N} |y_{i} -  \\hat{y_{i}}|$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgTc8LFe5ndN"
      },
      "outputs": [],
      "source": [
        "# calculate Mean Absolute Error\n",
        "\n",
        "print(metrics.mean_absolute_error(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh2V1f7h5ndO"
      },
      "source": [
        "#### **MSE - Mean Squared Error** ou **Erro Médio Quadrático**\n",
        "\n",
        "- Tira a média do quadrado da diferença entre os valores originais e os valores previstos.\n",
        "- À medida que tomamos o quadrado do erro, o efeito de erros maiores (às vezes discrepantes) torna-se mais pronunciado do que o erro menor. O modelo será mais penalizado por fazer previsões que diferem muito do valor real correspondente.\n",
        "- Antes de aplicar o MSE, devemos eliminar todos os nulos/infinitos da entrada.\n",
        "- Não é robusto a outliers\n",
        "- Alcance (0, + infinito]\n",
        "\n",
        "$$ Mean\\ Squared\\ Error = \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} -  \\hat{y_{i}})^2$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1clgOW95ndO"
      },
      "outputs": [],
      "source": [
        "# calculo do Mean Squared Error\n",
        "print(metrics.mean_squared_error(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdme7FIW5ndO"
      },
      "source": [
        "#### **MAE vs. MSE**\n",
        "- Sendo mais complexo e tendencioso para um desvio mais alto, o RMSE ainda é a métrica padrão de muitos modelos porque a função de perda definida em termos de RMSE é suavemente diferenciável, enquanto o MAE (Erro Absoluto Médio) requer programação linear complicada para calcular o gradiente.\n",
        "- Se queremos uma métrica apenas para comparar dois modelos do ponto de vista de interpretação, então o MAE pode ser uma escolha melhor.\n",
        "- As unidades de RMSE e MAE são iguais aos valores de y, o que não é verdade para R Square.\n",
        "- Minimizar o erro quadrado (𝐿2) sobre um conjunto de números resulta em encontrar sua média, e minimizar o erro absoluto (𝐿1) resulta em encontrar sua mediana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGZwZQ7h5ndO"
      },
      "source": [
        "#### **RMSE**\n",
        "\n",
        "- Como o MSE é quadrático, suas unidades não correspondem às da saída original. RMSE é a raiz quadrada de MSE.\n",
        "- Uma vez que o MSE e o RMSE ambos quadram o resíduo, eles são afetados da mesma forma por valores discrepantes.\n",
        "- O RMSE é análogo ao desvio padrão e é uma medida do tamanho dos resíduos espalhados.\n",
        "- - Geralmente, o RMSE será maior ou igual ao MAE.\n",
        "\n",
        "$$ Root\\ Mean\\ Squared\\ Error =\\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (y_{i} -  \\hat{y_{i}})^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFCCMc4U5ndO"
      },
      "outputs": [],
      "source": [
        "# calculate Root Mean Squared Error\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "print(sqrt(metrics.mean_squared_error(y_test, y_pred_class)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiGnd-eO5ndP"
      },
      "source": [
        "#### **RMSLE - Root Mean Squared Logarithmic Error**\n",
        "- Tomamos o log das previsões e valores reais.\n",
        "- Quais mudanças são a variância que estamos medindo.\n",
        "- O RMSLE geralmente é usado quando não queremos penalizar grandes diferenças nos valores previstos e reais quando os valores previstos e reais são números enormes.\n",
        "- Se os valores previstos e reais forem pequenos: RMSE e RMSLE são iguais.\n",
        "- Se o valor previsto ou real for grande: RMSE > RMSLE\n",
        "- Se os valores previstos e reais forem grandes: RMSE > RMSLE (RMSLE torna-se quase insignificante)\n",
        "\n",
        "$$ Root\\ Mean\\ Squared\\ Log\\ Error =\\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} (\\log (y_{i} + 1) -  (\\log \\hat{y_{i}} + 1))^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9KNfl9-5ndP"
      },
      "outputs": [],
      "source": [
        "# calculate Mean Squared Log Error\n",
        "\n",
        "print(metrics.mean_squared_log_error(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Osbsyei5ndP"
      },
      "source": [
        "#### **R-squared**\n",
        "\n",
        "No caso de um problema de classificação, se o modelo tiver uma precisão de 0,8, podemos avaliar o quão bom nosso modelo é em relação a um modelo aleatório, que tem uma precisão de 0,5. Assim, o modelo aleatório pode ser tratado como um benchmark. Mas quando falamos em métricas de RMSE, não temos um benchmark para comparar.\n",
        "\n",
        "É aqui que podemos usar a métrica R-squared. A fórmula para R-squared é a seguinte:\n",
        "\n",
        "$$R^2 = 1 - \\frac{MSE(model)}{MSE(baseline)} = 1 - \\frac{\\sum_{i=1}^{N}(y_1 - \\hat{y_1})^2}{\\sum_{i=1}^{N}(\\bar{y_1} - \\hat{y_1})^2}$$\n",
        "\n",
        "MSE(model): Mean Squared Error of the predictions against the actual values\n",
        "\n",
        "MSE(baseline): Mean Squared Error of  mean prediction against the actual values\n",
        "\n",
        "In other words how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.\n",
        "- A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value.\n",
        "- Range[- infinity, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VqTymwd5ndQ"
      },
      "outputs": [],
      "source": [
        "# calculate R2 score\n",
        "\n",
        "print(metrics.r2_score(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BU3mF025ndQ"
      },
      "source": [
        "#### **Adjusted R-Squared**\n",
        "\n",
        "Ao adicionar novos recursos ao modelo, o valor R-Squared aumenta ou permanece o mesmo. O R-Squared não penaliza a adição de recursos que não agregam valor ao modelo. Portanto, uma versão melhorada sobre o R-Squared é o R-Squared ajustado. A fórmula para R-Quadrático ajustado é dada por:\n",
        "\n",
        "$$\\bar{R^2} = 1 - (1 - R^2)(\\frac{n - 1}{n - k + 1})$$\n",
        "\n",
        "- k: number of features - número de features\n",
        "- n: number of samples - número de exemplos\n",
        "\n",
        "Essa métrica leva em consideração o número de recursos. Quando adicionamos mais recursos, o termo no denominador n-(k +1) diminui, então toda a expressão aumenta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpaAOH2Z5ndQ"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "X_train_2 = sm.add_constant(X_train)\n",
        "est = sm.OLS(y_train, X_train_2)\n",
        "est2 = est.fit()\n",
        "\n",
        "print(\"summary()\\n\",est2.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Notebook utilizado para fins educacionais da **Awari**.\n",
        "\n",
        "**© AWARI. Todos os direitos reservados.**"
      ],
      "metadata": {
        "id": "PEUwtERx9ALH"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nXkxaYu75ndB",
        "8Y1FJKzQ5ndC",
        "P_aBqdJm5ndM",
        "wKo7etJT5ndN",
        "Zdme7FIW5ndO",
        "RGZwZQ7h5ndO",
        "GiGnd-eO5ndP",
        "9Osbsyei5ndP"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}